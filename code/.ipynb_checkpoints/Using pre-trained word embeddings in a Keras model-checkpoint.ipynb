{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from preprocess_tweets import preprocess_tweet, generate_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, embedding_matrix = generate_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIMENSION = 25\n",
    "MAX_WORDS = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []  \n",
    "test_tweets = []  \n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tweets proccessed.\n",
      "Total of 7613 train tweets.\n",
      "Total of 3263 test tweets.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the train tweets\n",
    "for row in df_train.iterrows():\n",
    "    tweet = preprocess_tweet(row[1]['text'])\n",
    "    train_tweets.append(tweet)\n",
    "    labels.append(row[1]['target'])\n",
    "    \n",
    "# Preprocessing the train tweets\n",
    "for row in df_test.iterrows():\n",
    "    tweet = preprocess_tweet(row[1]['text'])\n",
    "    test_tweets.append(tweet)\n",
    "    \n",
    "print('Test tweets proccessed.')\n",
    "print('Total of %s train tweets.' % len(train_tweets))\n",
    "print('Total of %s test tweets.' % len(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulding the vocabulary...\n"
     ]
    }
   ],
   "source": [
    "# Mapping every unique word to a integer (bulding the vocabulary)\n",
    "print('Bulding the vocabulary...')\n",
    "word_to_index = {}\n",
    "words_freq = {}\n",
    "m = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(train_tweets):\n",
    "    words = tweet.split()\n",
    "        \n",
    "    for word in words[:MAX_WORDS]:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = m\n",
    "            m += 1\n",
    "        if word not in words_freq:\n",
    "            words_freq[word] = 1\n",
    "        else:\n",
    "            words_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulding the vocabulary done, vocabulary size: 15704.\n"
     ]
    }
   ],
   "source": [
    "word_to_index[\"unk\"] = m\n",
    "vocabulary_size = len(word_to_index)\n",
    "print('Bulding the vocabulary done, vocabulary size: %s.' % vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training tweets to integer sequences...\n"
     ]
    }
   ],
   "source": [
    "print('Converting training tweets to integer sequences...')\n",
    "train_sequences = []\n",
    "\n",
    "for i, tweet in enumerate(train_tweets):\n",
    "    words = tweet.split()\n",
    "\n",
    "    tweet_seq = []\n",
    "    for word in words[:MAX_WORDS]:\n",
    "        if word not in word_to_index:\n",
    "            tweet_seq.append(word_to_index[\"unk\"])\n",
    "        else:\n",
    "            tweet_seq.append(word_to_index[word])\n",
    "\n",
    "    train_sequences.append(tweet_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion done.\n",
      "(7613, 30)\n"
     ]
    }
   ],
   "source": [
    "# Padding the sequences to match the `MAX_WORDS`\n",
    "X_train = pad_sequences(train_sequences, maxlen=MAX_WORDS, padding=\"post\", value=vocabulary_size)\n",
    "print('Conversion done.')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting testing tweets to integer sequences...\n"
     ]
    }
   ],
   "source": [
    "print('Converting testing tweets to integer sequences...')\n",
    "test_sequences = []\n",
    "\n",
    "for i, tweet in enumerate(test_tweets):\n",
    "    words = tweet.split()\n",
    "\n",
    "    tweet_seq = []\n",
    "    for word in words[:MAX_WORDS]:\n",
    "        if word not in word_to_index:\n",
    "            tweet_seq.append(word_to_index[\"unk\"])\n",
    "        else:\n",
    "            tweet_seq.append(word_to_index[word])\n",
    "\n",
    "    test_sequences.append(tweet_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion done.\n",
      "(3263, 30)\n"
     ]
    }
   ],
   "source": [
    "# Padding the sequences to match the `MAX_WORDS`\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_WORDS, padding=\"post\", value=vocabulary_size)\n",
    "print('Conversion done.')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading glove embeddings...\n",
      "Done reading embeddings\n"
     ]
    }
   ],
   "source": [
    "print('Reading glove embeddings...')\n",
    "glove_embeddings_file = open('../data/glove.twitter.27B.25d.txt', 'r', encoding='UTF-8')\n",
    "\n",
    "glove_embeddings = dict()\n",
    "for line in glove_embeddings_file:\n",
    "    parts = line.split()\n",
    "    key = parts[0]\n",
    "    embedding = [float(t) for t in parts[1:]]\n",
    "    glove_embeddings[key] = np.array(embedding)\n",
    "print (\"Done reading embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the embedding matrix...\n"
     ]
    }
   ],
   "source": [
    "# Generating the embedding matrix for our vocabulary (this is needed for the Embedding layer in keras models)\n",
    "print('Generating the embedding matrix...')\n",
    "unknown = []\n",
    "hits = 0\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, GLOVE_DIMENSION))\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        emb = glove_embeddings[word]\n",
    "        embedding_matrix[idx] = emb\n",
    "        hits += 1\n",
    "    else:\n",
    "        unknown.append(word)\n",
    "        emb = glove_embeddings[\"unk\"]\n",
    "        embedding_matrix[idx] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating done.\n",
      "12347 words of 15704 found\n",
      "(15705, 25)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix[vocabulary_size] = [0]*GLOVE_DIMENSION\n",
    "print('Generating done.')\n",
    "print('%s words of %s found' % (hits, vocabulary_size))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
