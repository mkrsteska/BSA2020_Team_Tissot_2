{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCsgJOIydd3p1f+izgEvXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkrsteska/BSA2020_Team_Tissot_Project_2/blob/master/code/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBJ-CXhWhQ5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAzyhYELhFD",
        "colab_type": "text"
      },
      "source": [
        "I tried using a different data set of fake&real news to see if a model trained on that could be used on the twitter database\n",
        "\n",
        "**It does not work :(**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6kic62hVtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_news = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/real_news.csv\",\n",
        "                        sep =';',\n",
        "                        encoding='utf_8', \n",
        "                        dtype = 'unicode',\n",
        "                        low_memory= False)\n",
        "\n",
        "fake_news = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/fake_news.csv\", \n",
        "                        sep =';',\n",
        "                        encoding='utf_8', \n",
        "                        dtype = 'unicode',\n",
        "                        low_memory= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-A9S0YlhgYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/DLaux/BSA2020_Team_Tissot_Project_2/master/data/train.csv\", \n",
        "                          encoding='utf_8', \n",
        "                          dtype = 'unicode',\n",
        "                          parse_dates = True,\n",
        "                          infer_datetime_format = True,\n",
        "                          low_memory=False)\n",
        "\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/DLaux/BSA2020_Team_Tissot_Project_2/master/data/test.csv\", \n",
        "                          encoding='utf_8', \n",
        "                          dtype = 'unicode',\n",
        "                          parse_dates = True,\n",
        "                          infer_datetime_format = True,\n",
        "                          low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n51HkZ1oofqP",
        "colab_type": "code",
        "outputId": "a0dbe534-7ac4-4cfe-eb70-4732ddf73ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id keyword location                                               text target\n",
              "0  1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1  4     NaN      NaN             Forest fire near La Ronge Sask. Canada      1\n",
              "2  5     NaN      NaN  All residents asked to 'shelter in place' are ...      1\n",
              "3  6     NaN      NaN  13,000 people receive #wildfires evacuation or...      1\n",
              "4  7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u3yawd-pkMG",
        "colab_type": "code",
        "outputId": "ccd83cca-1e25-410d-9002-dc99039dd78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "df_train.groupby([\"target\"]).count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4342</td>\n",
              "      <td>4323</td>\n",
              "      <td>2884</td>\n",
              "      <td>4342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3271</td>\n",
              "      <td>3229</td>\n",
              "      <td>2196</td>\n",
              "      <td>3271</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  keyword  location  text\n",
              "target                               \n",
              "0       4342     4323      2884  4342\n",
              "1       3271     3229      2196  3271"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLq6UzyohWi",
        "colab_type": "code",
        "outputId": "6f21b71d-0a29-41dd-c882-a37a16797284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "r1 = df_train[[\"id\",\"target\"]].groupby([\"target\"]).count()\n",
        "r1.plot.bar(x=None, \n",
        "            y=None,\n",
        "            figsize=(15,5), \n",
        "            alpha = 0.8, # make the plot 20% transparent\n",
        "            legend = None, \n",
        "           )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fece1a1a2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzKLzf60p9X4",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSNeWB3sp7jn",
        "colab_type": "code",
        "outputId": "d30e42e4-b8a1-4ca0-e35b-937b3367d1cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdSXePyWvrKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXMKK4M7qH7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df_train['text'].astype('str')\n",
        "y = df_train[\"target\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gutP9CPuqoH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encode the target \n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "encoded_y = lab_enc.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSNwXcRBqzY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=72, stratify=encoded_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VFS-pCMrpYp",
        "colab_type": "text"
      },
      "source": [
        "### Text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7j2CN31wf4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
        "VAL_SIZE = 1000  # Size of the validation set\n",
        "NB_START_EPOCHS = 30  # Number of epochs we usually start to train with\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 28  # Maximum number of words in a sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GY7K-SdwX6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Function to remove English stopwords from a Pandas Series.'''\n",
        "def remove_stopwords(input_text):\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    words = input_text.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "    return \" \".join(clean_words) \n",
        "    \n",
        "'''Function to remove mentions, preceded by @, in a Pandas Series'''\n",
        "def remove_mentions(input_text):\n",
        "    return re.sub(r'@\\w+', '', input_text)\n",
        "\n",
        "def remove_URL(input_text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',input_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1qrQPA-KGDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.apply(lambda x: remove_URL(x)).apply(lambda x: remove_stopwords(x)).apply(lambda x: remove_mentions(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR8qDrnutnr0",
        "colab_type": "text"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoFEWX_frsqC",
        "colab_type": "code",
        "outputId": "7d642831-0933-4479-f16c-fd7be1c0ef32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer(min_df=2, max_df=0.75, encoding='utf-8')\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "X_train_counts.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6090, 5499)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5G140W9tqht",
        "colab_type": "text"
      },
      "source": [
        "Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CpdT4K8tmG3",
        "colab_type": "code",
        "outputId": "aa60e428-106e-44e5-8061-6d41efe3eaaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6090, 5499)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAtX7eott7w",
        "colab_type": "text"
      },
      "source": [
        "This is just to test how effectif these methods are for our data set, later on we will do every step in a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS5Ycmbe_6sE",
        "colab_type": "code",
        "outputId": "2ecbab06-1de3-4c02-9e5d-3b0ddfc459df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipe = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.75, encoding='utf-8')),\n",
        "                 ('tfidf', TfidfTransformer()),\n",
        "                 ('model', RandomForestClassifier())])\n",
        "\n",
        "model = pipe.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.91      0.83       869\n",
            "           1       0.84      0.63      0.72       654\n",
            "\n",
            "    accuracy                           0.79      1523\n",
            "   macro avg       0.80      0.77      0.77      1523\n",
            "weighted avg       0.80      0.79      0.78      1523\n",
            "\n",
            "[[790  79]\n",
            " [243 411]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GqckDsRrLo9",
        "colab_type": "text"
      },
      "source": [
        "## Selecting a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxu_OSlNrPLX",
        "colab_type": "code",
        "outputId": "4a9479d6-2b7a-4814-e590-42f65693437e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classifiers = [   \n",
        "    KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
        "    SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
        "    NuSVC(probability=True, random_state=42),\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    AdaBoostClassifier(random_state=42),\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    MultinomialNB(),\n",
        "    SGDClassifier(random_state=42),\n",
        "    MLPClassifier(random_state=42)\n",
        "    ]\n",
        "\n",
        "for classifier in classifiers:\n",
        "    pipe = Pipeline([\n",
        "                     ('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('smote', SMOTE()),\n",
        "                     ('classifier', classifier)\n",
        "                     ])\n",
        "    pipe.fit(X_train, y_train)   \n",
        "    print(classifier)\n",
        "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
        "    \n",
        "    predictions = pipe.predict(X_test)\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
            "                     weights='distance')\n",
            "model score: 0.444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.04      0.08       869\n",
            "           1       0.43      0.98      0.60       654\n",
            "\n",
            "    accuracy                           0.44      1523\n",
            "   macro avg       0.58      0.51      0.34      1523\n",
            "weighted avg       0.60      0.44      0.30      1523\n",
            "\n",
            "[[ 35 834]\n",
            " [ 13 641]]\n",
            "SVC(C=0.025, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "model score: 0.571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.73       869\n",
            "           1       0.00      0.00      0.00       654\n",
            "\n",
            "    accuracy                           0.57      1523\n",
            "   macro avg       0.29      0.50      0.36      1523\n",
            "weighted avg       0.33      0.57      0.41      1523\n",
            "\n",
            "[[869   0]\n",
            " [654   0]]\n",
            "NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "      decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "      max_iter=-1, nu=0.5, probability=True, random_state=42, shrinking=True,\n",
            "      tol=0.001, verbose=False)\n",
            "model score: 0.786\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.93      0.83       869\n",
            "           1       0.86      0.60      0.71       654\n",
            "\n",
            "    accuracy                           0.79      1523\n",
            "   macro avg       0.81      0.76      0.77      1523\n",
            "weighted avg       0.80      0.79      0.78      1523\n",
            "\n",
            "[[804  65]\n",
            " [261 393]]\n",
            "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=42, splitter='best')\n",
            "model score: 0.704\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.74      0.74       869\n",
            "           1       0.65      0.66      0.66       654\n",
            "\n",
            "    accuracy                           0.70      1523\n",
            "   macro avg       0.70      0.70      0.70      1523\n",
            "weighted avg       0.70      0.70      0.70      1523\n",
            "\n",
            "[[639 230]\n",
            " [221 433]]\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
            "                       warm_start=False)\n",
            "model score: 0.781\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82       869\n",
            "           1       0.81      0.64      0.71       654\n",
            "\n",
            "    accuracy                           0.78      1523\n",
            "   macro avg       0.79      0.76      0.77      1523\n",
            "weighted avg       0.79      0.78      0.78      1523\n",
            "\n",
            "[[774  95]\n",
            " [238 416]]\n",
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=50, random_state=42)\n",
            "model score: 0.743\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.91      0.80       869\n",
            "           1       0.81      0.53      0.64       654\n",
            "\n",
            "    accuracy                           0.74      1523\n",
            "   macro avg       0.76      0.72      0.72      1523\n",
            "weighted avg       0.76      0.74      0.73      1523\n",
            "\n",
            "[[788  81]\n",
            " [310 344]]\n",
            "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                           n_iter_no_change=None, presort='deprecated',\n",
            "                           random_state=42, subsample=1.0, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False)\n",
            "model score: 0.747\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.91      0.80       869\n",
            "           1       0.82      0.53      0.64       654\n",
            "\n",
            "    accuracy                           0.75      1523\n",
            "   macro avg       0.77      0.72      0.72      1523\n",
            "weighted avg       0.76      0.75      0.73      1523\n",
            "\n",
            "[[793  76]\n",
            " [310 344]]\n",
            "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
            "model score: 0.789\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.82       869\n",
            "           1       0.79      0.70      0.74       654\n",
            "\n",
            "    accuracy                           0.79      1523\n",
            "   macro avg       0.79      0.78      0.78      1523\n",
            "weighted avg       0.79      0.79      0.79      1523\n",
            "\n",
            "[[744 125]\n",
            " [197 457]]\n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "model score: 0.785\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.83       869\n",
            "           1       0.82      0.64      0.72       654\n",
            "\n",
            "    accuracy                           0.78      1523\n",
            "   macro avg       0.79      0.77      0.77      1523\n",
            "weighted avg       0.79      0.78      0.78      1523\n",
            "\n",
            "[[774  95]\n",
            " [233 421]]\n",
            "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=42, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)\n",
            "model score: 0.748\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.83      0.79       869\n",
            "           1       0.74      0.64      0.69       654\n",
            "\n",
            "    accuracy                           0.75      1523\n",
            "   macro avg       0.75      0.73      0.74      1523\n",
            "weighted avg       0.75      0.75      0.74      1523\n",
            "\n",
            "[[720 149]\n",
            " [235 419]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K4LN-bP1iPu",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFGZWpsZ1h1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKPVqZs71vcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_MLP = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('smote', SMOTE()),\n",
        "                     ('mlp', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 10), random_state=1, max_iter= 1000)),])\n",
        "text_MLP = text_MLP.fit(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_a1clx2KJj",
        "colab_type": "code",
        "outputId": "c9c7346e-a4f9-4f1a-b45f-f9f7884dbd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "predictions = text_MLP.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.79       869\n",
            "           1       0.73      0.70      0.72       654\n",
            "\n",
            "    accuracy                           0.76      1523\n",
            "   macro avg       0.76      0.75      0.76      1523\n",
            "weighted avg       0.76      0.76      0.76      1523\n",
            "\n",
            "[[700 169]\n",
            " [194 460]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugGMSVqY7TzQ",
        "colab_type": "code",
        "outputId": "8f6f5e78-eb1a-4e42-fe43-04698d6668ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "\n",
        "\n",
        "parameters = {\n",
        "    #'vect__analyzer' : ('word', 'char'),\n",
        "    #'vect__max_df': (0.75, 0.80, 0.85),\n",
        "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
        "    \n",
        "    'mlp__hidden_layer_sizes': [(100, 10),(100, 50),(200, 50),(50, 10)]\n",
        "}\n",
        "\n",
        "\n",
        "CV = GridSearchCV(text_MLP, parameters, n_jobs= 1)\n",
        "                  \n",
        "CV.fit(X_train, y_train)  \n",
        "print(CV.best_params_) \n",
        "print(CV.best_score_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'mlp__hidden_layer_sizes': (100, 50)}\n",
            "0.7663382594417077\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}