{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 30  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 28  # Maximum number of words in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to remove English stopwords from a Pandas Series.'''\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "    \n",
    "'''Function to remove mentions, preceded by @, in a Pandas Series'''\n",
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.target, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6851.000000\n",
       "mean       10.749088\n",
       "std         3.881148\n",
       "min         1.000000\n",
       "25%         8.000000\n",
       "50%        11.000000\n",
       "75%        14.000000\n",
       "max        28.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 28, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 450       \n",
      "=================================================================\n",
      "Total params: 80,450\n",
      "Trainable params: 80,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
    "emb_model.add(layers.Flatten())\n",
    "emb_model.add(layers.Dense(2, activation='softmax'))\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6165 samples, validate on 686 samples\n",
      "Epoch 1/30\n",
      "6165/6165 [==============================] - 0s 76us/step - loss: 0.6891 - accuracy: 0.5515 - val_loss: 0.6857 - val_accuracy: 0.5408\n",
      "Epoch 2/30\n",
      "6165/6165 [==============================] - 0s 12us/step - loss: 0.6687 - accuracy: 0.5935 - val_loss: 0.6801 - val_accuracy: 0.5204\n",
      "Epoch 3/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.6497 - accuracy: 0.5916 - val_loss: 0.6739 - val_accuracy: 0.5292\n",
      "Epoch 4/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.6308 - accuracy: 0.6212 - val_loss: 0.6619 - val_accuracy: 0.5714\n",
      "Epoch 5/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.6068 - accuracy: 0.6976 - val_loss: 0.6406 - val_accuracy: 0.6487\n",
      "Epoch 6/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.5817 - accuracy: 0.7593 - val_loss: 0.6212 - val_accuracy: 0.6764\n",
      "Epoch 7/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.5563 - accuracy: 0.7809 - val_loss: 0.6059 - val_accuracy: 0.6822\n",
      "Epoch 8/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.5286 - accuracy: 0.7948 - val_loss: 0.5931 - val_accuracy: 0.6997\n",
      "Epoch 9/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.5018 - accuracy: 0.8104 - val_loss: 0.5771 - val_accuracy: 0.7157\n",
      "Epoch 10/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.4751 - accuracy: 0.8251 - val_loss: 0.5623 - val_accuracy: 0.7274\n",
      "Epoch 11/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.4486 - accuracy: 0.8441 - val_loss: 0.5483 - val_accuracy: 0.7391\n",
      "Epoch 12/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.4227 - accuracy: 0.8569 - val_loss: 0.5368 - val_accuracy: 0.7420\n",
      "Epoch 13/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.3977 - accuracy: 0.8680 - val_loss: 0.5243 - val_accuracy: 0.7464\n",
      "Epoch 14/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.3735 - accuracy: 0.8795 - val_loss: 0.5135 - val_accuracy: 0.7638\n",
      "Epoch 15/30\n",
      "6165/6165 [==============================] - 0s 9us/step - loss: 0.3510 - accuracy: 0.8916 - val_loss: 0.5056 - val_accuracy: 0.7755\n",
      "Epoch 16/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.3303 - accuracy: 0.8976 - val_loss: 0.4977 - val_accuracy: 0.7741\n",
      "Epoch 17/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.3111 - accuracy: 0.9079 - val_loss: 0.4897 - val_accuracy: 0.7770\n",
      "Epoch 18/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.2933 - accuracy: 0.9147 - val_loss: 0.4847 - val_accuracy: 0.7770\n",
      "Epoch 19/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.2770 - accuracy: 0.9181 - val_loss: 0.4808 - val_accuracy: 0.7857\n",
      "Epoch 20/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.2622 - accuracy: 0.9231 - val_loss: 0.4777 - val_accuracy: 0.7886\n",
      "Epoch 21/30\n",
      "6165/6165 [==============================] - 0s 12us/step - loss: 0.2484 - accuracy: 0.9299 - val_loss: 0.4742 - val_accuracy: 0.7886\n",
      "Epoch 22/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.2360 - accuracy: 0.9346 - val_loss: 0.4741 - val_accuracy: 0.7915\n",
      "Epoch 23/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.2243 - accuracy: 0.9361 - val_loss: 0.4735 - val_accuracy: 0.7930\n",
      "Epoch 24/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.2134 - accuracy: 0.9436 - val_loss: 0.4707 - val_accuracy: 0.7959\n",
      "Epoch 25/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.2033 - accuracy: 0.9471 - val_loss: 0.4716 - val_accuracy: 0.7945\n",
      "Epoch 26/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.1937 - accuracy: 0.9483 - val_loss: 0.4719 - val_accuracy: 0.7959\n",
      "Epoch 27/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.1850 - accuracy: 0.9523 - val_loss: 0.4722 - val_accuracy: 0.7959\n",
      "Epoch 28/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.1768 - accuracy: 0.9544 - val_loss: 0.4740 - val_accuracy: 0.7959\n",
      "Epoch 29/30\n",
      "6165/6165 [==============================] - 0s 10us/step - loss: 0.1691 - accuracy: 0.9557 - val_loss: 0.4756 - val_accuracy: 0.7974\n",
      "Epoch 30/30\n",
      "6165/6165 [==============================] - 0s 11us/step - loss: 0.1618 - accuracy: 0.9586 - val_loss: 0.4762 - val_accuracy: 0.7959\n"
     ]
    }
   ],
   "source": [
    "emb_model.compile(optimizer = 'adam', \n",
    "                  loss = 'binary_crossentropy', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "emb_history = emb_model.fit(X_train_emb, \n",
    "                            y_train_emb, \n",
    "                            epochs = NB_START_EPOCHS,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            validation_data = (X_valid_emb, y_valid_emb),\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762/762 [==============================] - 0s 21us/step\n",
      "Test accuracy of word embeddings model: 79.13%\n"
     ]
    }
   ],
   "source": [
    "emb_model.fit(X_train_seq_trunc\n",
    "              , y_train_oh\n",
    "              , epochs=6\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "\n",
    "emb_results = emb_model.evaluate(X_test_seq_trunc, y_test_oh)\n",
    "    \n",
    "print('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_le = emb_model.predict_classes(X_test_seq_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.802     0.851     0.826       443\n",
      "           1      0.774     0.708     0.740       319\n",
      "           2      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.791     0.791     0.791       762\n",
      "   macro avg      0.525     0.520     0.522       762\n",
      "weighted avg      0.790     0.791     0.790       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test_le, y_pred_le, labels=[0, 1, 2], digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
