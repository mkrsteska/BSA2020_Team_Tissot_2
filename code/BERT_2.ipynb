{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkrsteska/BSA2020_Team_Tissot_Project_2/blob/master/code/BERT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wVoD-i_TdpZD"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RkbUD1lBddt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F2vmlZmuD3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZvQJnDnyFxh",
        "colab_type": "text"
      },
      "source": [
        "#### BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckLHTGCPsvGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\", trainable = True)\n",
        "\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqMnF8BjyH4_",
        "colab_type": "text"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7UyVzRB_giic",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xYiaOXXnV8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = df_train.text.values\n",
        "y_train = df_train.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ2QU5FyHkpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmC9aszJK4ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_word_ids = np.array(all_input_word_ids)\n",
        "all_masks = np.array(all_masks)\n",
        "all_segment_ids = np.array(all_segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tatYh-g4z9Pn",
        "colab_type": "text"
      },
      "source": [
        "## Model #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9DJgpY_IK0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the tweets\n",
        "tokenized_tweets = [tokenizer.tokenize(tweet) for tweet in X_train]\n",
        "tokenized_tweets = [tweet[:MAX_LEN-2] for tweet in tokenized_tweets]\n",
        "\n",
        "# add special tokens at the beginning and end of each tweet for BERT to work properly\n",
        "tokenized_tweets = [[\"[CLS]\"] + tweet + [\"[SEP]\"] for tweet in tokenized_tweets]\n",
        "\n",
        "# convert the tokens to their index numbers in the BERT vocabulary\n",
        "tokenized_tweets_ids = [tokenizer.convert_tokens_to_ids(tweet) for tweet in tokenized_tweets]\n",
        "\n",
        "# pad the input tokens to max_len\n",
        "all_input_word_ids = [tweet + [0]*(MAX_LEN-len(tweet)) for tweet in tokenized_tweets_ids]\n",
        "# all_input_word_ids = pad_sequences(tokenized_tweets_ids, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\")\n",
        "\n",
        "# 1 - tokenized words, 0 - padded zeros\n",
        "all_masks = [[1] * len(tweet) + [0] * (MAX_LEN - len(tweet)) for tweet in tokenized_tweets]\n",
        "\n",
        "all_segment_ids = [[0] * MAX_LEN for tweet in tokenized_tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSYcUS5xKFIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zif78bM7--T_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "out = Dense(1, activation='sigmoid')(sequence_output[:, 0, :])\n",
        "    \n",
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkQlPbOL_EGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ-1vZZr-usx",
        "colab_type": "code",
        "outputId": "2b89939e-d5bc-4df8-ef04-23d8518e3607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QJohawCRYPi",
        "colab_type": "code",
        "outputId": "27570630-986d-43fe-b271-ac8d9877fd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model.fit(\n",
        "    (all_input_word_ids, all_masks, all_segment_ids), y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=2,\n",
        "    batch_size=4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1523/1523 [==============================] - 530s 348ms/step - loss: 0.4285 - accuracy: 0.8218 - val_loss: 0.3912 - val_accuracy: 0.8359\n",
            "Epoch 2/2\n",
            "1523/1523 [==============================] - 527s 346ms/step - loss: 0.2527 - accuracy: 0.9049 - val_loss: 0.3962 - val_accuracy: 0.8319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f37c1afe320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGKlAii-wD5D",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenize the tweets from the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV3NMQ7hwF9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHmBEtF7xPbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = df_test.text.values\n",
        "ids = df_test['id'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47VIho6wm9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the tweets\n",
        "tokenized_tweets_test = [tokenizer.tokenize(tweet) for tweet in X_test]\n",
        "tokenized_tweets_test = [tweet[:MAX_LEN-2] for tweet in tokenized_tweets_test]\n",
        "\n",
        "# add special tokens at the beginning and end of each tweet for BERT to work properly\n",
        "tokenized_tweets_test = [[\"[CLS]\"] + tweet + [\"[SEP]\"] for tweet in tokenized_tweets_test]\n",
        "\n",
        "# convert the tokens to their index numbers in the BERT vocabulary\n",
        "tokenized_tweets_ids_test = [tokenizer.convert_tokens_to_ids(tweet) for tweet in tokenized_tweets_test]\n",
        "\n",
        "# pad the input tokens to max_len\n",
        "all_input_word_ids_test = [tweet + [0]*(MAX_LEN-len(tweet)) for tweet in tokenized_tweets_ids_test]\n",
        "# all_input_word_ids = pad_sequences(tokenized_tweets_ids, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\")\n",
        "\n",
        "# 1 - tokenized words, 0 - padded zeros\n",
        "all_masks_test = [[1] * len(tweet) + [0] * (MAX_LEN - len(tweet)) for tweet in tokenized_tweets_test]\n",
        "\n",
        "all_segment_ids_test = [[0] * MAX_LEN for tweet in tokenized_tweets_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts-Yz4NTxW7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_word_ids_test = np.array(all_input_word_ids_test)\n",
        "all_masks_test = np.array(all_masks_test)\n",
        "all_segment_ids_test = np.array(all_segment_ids_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsvPzsqQ9FRW",
        "colab_type": "text"
      },
      "source": [
        "**Create a submission file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0N5wuuPxkxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict((all_input_word_ids_test, all_masks_test, all_segment_ids_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHIfnsGuuYsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_class = predictions.round().astype(int)[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHK6yl0xyS4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame({'id': ids, 'target': predictions_class}).to_csv('16. Submission_BERT_2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZruOSz749Ht2",
        "colab_type": "text"
      },
      "source": [
        "**Score** 0.84253"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgzKZZT_yL62",
        "colab_type": "text"
      },
      "source": [
        "## Model #2\n",
        "preprocess tweets and different learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKsvI7PeyN6-",
        "colab_type": "text"
      },
      "source": [
        "Correct mislabeled samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1zasrZjuR5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
        "df_train.loc[df_train['id'].isin(ids_with_target_error),'target'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roST12NW2lsh",
        "colab_type": "text"
      },
      "source": [
        "Preprocess tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1XnuloxyNPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from preprocess_tweets import preprocess_tweet_use\n",
        "\n",
        "X_train = df_train.text.apply(preprocess_tweet_use)\n",
        "y_train = df_train.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2SBk_vv0ZeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the tweets\n",
        "tokenized_tweets = [tokenizer.tokenize(tweet) for tweet in X_train]\n",
        "tokenized_tweets = [tweet[:MAX_LEN-2] for tweet in tokenized_tweets]\n",
        "\n",
        "# add special tokens at the beginning and end of each tweet for BERT to work properly\n",
        "tokenized_tweets = [[\"[CLS]\"] + tweet + [\"[SEP]\"] for tweet in tokenized_tweets]\n",
        "\n",
        "# convert the tokens to their index numbers in the BERT vocabulary\n",
        "tokenized_tweets_ids = [tokenizer.convert_tokens_to_ids(tweet) for tweet in tokenized_tweets]\n",
        "\n",
        "# pad the input tokens to max_len\n",
        "all_input_word_ids = [tweet + [0]*(MAX_LEN-len(tweet)) for tweet in tokenized_tweets_ids]\n",
        "# all_input_word_ids = pad_sequences(tokenized_tweets_ids, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\")\n",
        "\n",
        "# 1 - tokenized words, 0 - padded zeros\n",
        "all_masks = [[1] * len(tweet) + [0] * (MAX_LEN - len(tweet)) for tweet in tokenized_tweets]\n",
        "\n",
        "all_segment_ids = [[0] * MAX_LEN for tweet in tokenized_tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kVyOVXx0ujE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_word_ids = np.array(all_input_word_ids)\n",
        "all_masks = np.array(all_masks)\n",
        "all_segment_ids = np.array(all_segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVV3NWST0wip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdRMjb-h0zHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "out = Dense(1, activation='sigmoid')(sequence_output[:, 0, :])\n",
        "    \n",
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFNJL6Qu1Cgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_qNHIoU1hhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "bc22728f-b5ee-4543-8690-698e815911c2"
      },
      "source": [
        "model.fit(\n",
        "    (all_input_word_ids, all_masks, all_segment_ids), y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=3,\n",
        "    batch_size=4)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1523/1523 [==============================] - 555s 364ms/step - loss: 0.4334 - accuracy: 0.8108 - val_loss: 0.3840 - val_accuracy: 0.8378\n",
            "Epoch 2/3\n",
            "1523/1523 [==============================] - 554s 364ms/step - loss: 0.3272 - accuracy: 0.8686 - val_loss: 0.4151 - val_accuracy: 0.8437\n",
            "Epoch 3/3\n",
            "1523/1523 [==============================] - 549s 361ms/step - loss: 0.2479 - accuracy: 0.9033 - val_loss: 0.4584 - val_accuracy: 0.8326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f37c24335c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q83L1RB08wzE",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenize the tweets from the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxQKTH401mCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN0D_GLu4GyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = df_test.text.apply(preprocess_tweet_use)\n",
        "ids = df_test['id'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tm3USPN4Pd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the tweets\n",
        "tokenized_tweets_test = [tokenizer.tokenize(tweet) for tweet in X_test]\n",
        "tokenized_tweets_test = [tweet[:MAX_LEN-2] for tweet in tokenized_tweets_test]\n",
        "\n",
        "# add special tokens at the beginning and end of each tweet for BERT to work properly\n",
        "tokenized_tweets_test = [[\"[CLS]\"] + tweet + [\"[SEP]\"] for tweet in tokenized_tweets_test]\n",
        "\n",
        "# convert the tokens to their index numbers in the BERT vocabulary\n",
        "tokenized_tweets_ids_test = [tokenizer.convert_tokens_to_ids(tweet) for tweet in tokenized_tweets_test]\n",
        "\n",
        "# pad the input tokens to max_len\n",
        "all_input_word_ids_test = [tweet + [0]*(MAX_LEN-len(tweet)) for tweet in tokenized_tweets_ids_test]\n",
        "# all_input_word_ids = pad_sequences(tokenized_tweets_ids, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\")\n",
        "\n",
        "# 1 - tokenized words, 0 - padded zeros\n",
        "all_masks_test = [[1] * len(tweet) + [0] * (MAX_LEN - len(tweet)) for tweet in tokenized_tweets_test]\n",
        "\n",
        "all_segment_ids_test = [[0] * MAX_LEN for tweet in tokenized_tweets_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etcAxlaZ4Rm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_word_ids_test = np.array(all_input_word_ids_test)\n",
        "all_masks_test = np.array(all_masks_test)\n",
        "all_segment_ids_test = np.array(all_segment_ids_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lHvNE2Qy4Ac7"
      },
      "source": [
        "**Create a submission file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDFj8orG4TpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict((all_input_word_ids_test, all_masks_test, all_segment_ids_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gq05qh4UrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_class = predictions.round().astype(int)[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8jCnXCG4XFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame({'id': ids, 'target': predictions_class}).to_csv('17. Submission_BERT_2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWqjUh1H8sD3",
        "colab_type": "text"
      },
      "source": [
        "**Score** 0.84049"
      ]
    }
  ]
}