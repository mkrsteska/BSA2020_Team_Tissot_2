{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkrsteska/BSA2020_Team_Tissot_Project_2/blob/master/code/BERT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wVoD-i_TdpZD"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RkbUD1lBddt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "20f16191-eec7-4b79-f424-a7110c5891f1"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 3.9MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=c76f7d8a71b3ca96207096af725185e3cafc95e44ad96ee086247a32cc09f3d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=b41551ec8a1c0cfcb2adae5cb96bbb5a77f294ab660bf870faf9447f5c33eceb\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=e6d08ae5951eb34cb6906e26268b7e09dcfa100ebc625f49b761660681d50b87\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F2vmlZmuD3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZvQJnDnyFxh",
        "colab_type": "text"
      },
      "source": [
        "#### BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckLHTGCPsvGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable = True)\n",
        "\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqMnF8BjyH4_",
        "colab_type": "text"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7UyVzRB_giic",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xYiaOXXnV8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = df_train.text.values\n",
        "y_train = df_train.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYw2z4ABzx-c",
        "colab_type": "text"
      },
      "source": [
        "#### BERT encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ2QU5FyHkpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9DJgpY_IK0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the tweets\n",
        "tokenized_tweets = [tokenizer.tokenize(tweet) for tweet in X_train]\n",
        "tokenized_tweets = [tweet[:MAX_LEN-2] for tweet in tokenized_tweets]\n",
        "\n",
        "# add special tokens at the beginning and end of each tweet for BERT to work properly\n",
        "tokenized_tweets = [[\"[CLS]\"] + tweet + [\"[SEP]\"] for tweet in tokenized_tweets]\n",
        "\n",
        "# convert the tokens to their index numbers in the BERT vocabulary\n",
        "tokenized_tweets_ids = [tokenizer.convert_tokens_to_ids(tweet) for tweet in tokenized_tweets]\n",
        "\n",
        "# pad the input tokens to max_len\n",
        "all_input_word_ids = [tweet + [0]*(MAX_LEN-len(tweet)) for tweet in tokenized_tweets_ids]\n",
        "# all_input_word_ids = pad_sequences(tokenized_tweets_ids, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\")\n",
        "\n",
        "# 1 - tokenized words, 0 - padded zeros\n",
        "all_masks = [[1] * len(tweet) + [0] * (MAX_LEN - len(tweet)) for tweet in tokenized_tweets]\n",
        "\n",
        "all_segment_ids = [[0] * MAX_LEN for tweet in tokenized_tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmC9aszJK4ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_word_ids = np.array(all_input_word_ids)\n",
        "all_masks = np.array(all_masks)\n",
        "all_segment_ids = np.array(all_segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tatYh-g4z9Pn",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSYcUS5xKFIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zif78bM7--T_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "out = Dense(1, activation='sigmoid')(sequence_output[:, 0, :])\n",
        "    \n",
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkQlPbOL_EGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ-1vZZr-usx",
        "colab_type": "code",
        "outputId": "2b89939e-d5bc-4df8-ef04-23d8518e3607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QJohawCRYPi",
        "colab_type": "code",
        "outputId": "27570630-986d-43fe-b271-ac8d9877fd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model.fit(\n",
        "    (all_input_word_ids, all_masks, all_segment_ids), y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=2,\n",
        "    batch_size=4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1523/1523 [==============================] - 530s 348ms/step - loss: 0.4285 - accuracy: 0.8218 - val_loss: 0.3912 - val_accuracy: 0.8359\n",
            "Epoch 2/2\n",
            "1523/1523 [==============================] - 527s 346ms/step - loss: 0.2527 - accuracy: 0.9049 - val_loss: 0.3962 - val_accuracy: 0.8319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f37c1afe320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGKlAii-wD5D",
        "colab_type": "text"
      },
      "source": [
        "### Create a submission file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV3NMQ7hwF9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/mkrsteska/BSA2020_Team_Tissot_Project_2/master/data/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHmBEtF7xPbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = df_test.text.values\n",
        "ids = df_test['id'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47VIho6wm9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the tweets\n",
        "tokenized_tweets_test = [tokenizer.tokenize(tweet) for tweet in X_test]\n",
        "tokenized_tweets_test = [tweet[:MAX_LEN-2] for tweet in tokenized_tweets_test]\n",
        "\n",
        "# add special tokens at the beginning and end of each tweet for BERT to work properly\n",
        "tokenized_tweets_test = [[\"[CLS]\"] + tweet + [\"[SEP]\"] for tweet in tokenized_tweets_test]\n",
        "\n",
        "# convert the tokens to their index numbers in the BERT vocabulary\n",
        "tokenized_tweets_ids_test = [tokenizer.convert_tokens_to_ids(tweet) for tweet in tokenized_tweets_test]\n",
        "\n",
        "# pad the input tokens to max_len\n",
        "all_input_word_ids_test = [tweet + [0]*(MAX_LEN-len(tweet)) for tweet in tokenized_tweets_ids_test]\n",
        "# all_input_word_ids = pad_sequences(tokenized_tweets_ids, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\")\n",
        "\n",
        "# 1 - tokenized words, 0 - padded zeros\n",
        "all_masks_test = [[1] * len(tweet) + [0] * (MAX_LEN - len(tweet)) for tweet in tokenized_tweets_test]\n",
        "\n",
        "all_segment_ids_test = [[0] * MAX_LEN for tweet in tokenized_tweets_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts-Yz4NTxW7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_word_ids_test = np.array(all_input_word_ids_test)\n",
        "all_masks_test = np.array(all_masks_test)\n",
        "all_segment_ids_test = np.array(all_segment_ids_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0N5wuuPxkxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict((all_input_word_ids_test, all_masks_test, all_segment_ids_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHIfnsGuuYsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_class = predictions.round().astype(int)[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHK6yl0xyS4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame({'id': ids, 'target': predictions_class}).to_csv('16. Submission_BERT_2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1zasrZjuR5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}