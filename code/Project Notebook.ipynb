{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/DLaux/BSA2020_Team_Tissot_Project_2/master/data/train.csv\", \n",
    "                          encoding='utf_8', \n",
    "                          dtype = 'unicode',\n",
    "                          parse_dates = True,\n",
    "                          infer_datetime_format = True,\n",
    "                          low_memory=False)\n",
    "\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/DLaux/BSA2020_Team_Tissot_Project_2/master/data/test.csv\", \n",
    "                          encoding='utf_8', \n",
    "                          dtype = 'unicode',\n",
    "                          parse_dates = True,\n",
    "                          infer_datetime_format = True,\n",
    "                          low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocess_tweets import preprocess_tweet, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.text = df_train.text.apply(preprocess_tweet).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop([\"target\", \"id\"], axis =1)\n",
    "y = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keyword = X.keyword.astype(\"str\")\n",
    "X.location = X.location.astype(\"str\")\n",
    "X.text = X.text.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  keyword location                                               text\n",
      "0     nan      nan      deeds reason #earthquake may allah forgive us\n",
      "1     nan      nan             forest fire near la ronge sask. canada\n",
      "2     nan      nan  residents asked shelter place notified officer...\n",
      "3     nan      nan  <number> people receive #wildfires evacuation ...\n",
      "----------------------------------------------------------------------\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.head(4))\n",
    "print(\"-\"*70)\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the target \n",
    "lab_enc = LabelEncoder()\n",
    "encoded_y = lab_enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lab_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=72, stratify=encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_text = CountVectorizer(min_df=2, max_df=0.75, encoding='utf-8')\n",
    "X_train_text_counts = count_vect_text.fit_transform(X_train.text)\n",
    "\n",
    "\n",
    "#We don't need min_df and max_df for the rest of the features\n",
    "count_vect_location = CountVectorizer(encoding='utf-8')\n",
    "X_train_location_counts = count_vect_location.fit_transform(X_train.location.astype(\"str\"))\n",
    "\n",
    "count_vect_keyword = CountVectorizer(encoding='utf-8')\n",
    "X_train_keyword_counts = count_vect_keyword.fit_transform(X_train.keyword.astype(\"str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_text_tfidf = tfidf_transformer.fit_transform(X_train_text_counts)\n",
    "X_train_location_tfidf = tfidf_transformer.fit_transform(X_train_location_counts)\n",
    "X_train_keyword_tfidf = tfidf_transformer.fit_transform(X_train_keyword_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best')\n",
      "model score: 0.733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       869\n",
      "           1       0.68      0.71      0.70       654\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.73      0.73      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "[[652 217]\n",
      " [190 464]]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76       869\n",
      "           1       0.67      0.71      0.69       654\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.72      0.73      0.72      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "[[641 228]\n",
      " [187 467]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       869\n",
      "           1       0.67      0.71      0.69       654\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.72      0.72      0.72      1523\n",
      "weighted avg       0.73      0.72      0.73      1523\n",
      "\n",
      "[[636 233]\n",
      " [187 467]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    #NuSVC(probability=True, random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    #AdaBoostClassifier(random_state=42),\n",
    "    #GradientBoostingClassifier(random_state=42),\n",
    "    #MultinomialNB(),\n",
    "    SGDClassifier(random_state=42),\n",
    "    MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train.keyword, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test.keyword, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test.keyword)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It seems like that the DecisionTreeClassifier has the better model score without hyperparameter optimization.\n",
    "\n",
    "Top 3\n",
    "\n",
    "    DecisionTreeClassifier : 0.733\n",
    "    SGDClassifier :0.728\n",
    "    MLPClassifier : 0.724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.733\n"
     ]
    }
   ],
   "source": [
    "keyword_dtc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "keyword_dtc.fit(X_train.keyword, y_train)\n",
    "print(\"model score: %.3f\" % keyword_dtc.score(X_test.keyword, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       869\n",
      "           1       0.68      0.71      0.70       654\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.73      0.73      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "[[652 217]\n",
      " [190 464]]\n"
     ]
    }
   ],
   "source": [
    "predictions = keyword_dtc.predict(X_test.keyword)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dtc__max_depth': 85, 'dtc__min_samples_leaf': 1, 'dtc__min_samples_split': 2}\n",
      "0.7254515599343185\n"
     ]
    }
   ],
   "source": [
    "max_depth = [None, 85, 86, 87, 88, 89,90]\n",
    "min_samples_leaf = [None, 1, 2]\n",
    "min_samples_split = [2, 3, 4]\n",
    "\n",
    "parameters = {\n",
    "    \"dtc__max_depth\" : max_depth,\n",
    "    \"dtc__min_samples_split\" : min_samples_split,\n",
    "    \"dtc__min_samples_leaf\" : min_samples_leafs,\n",
    "}\n",
    "\n",
    "\n",
    "CV = GridSearchCV(keyword_dtc, parameters, n_jobs= 2)\n",
    "                  \n",
    "CV.fit(X_train.keyword, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.96      0.72       869\n",
      "           1       0.58      0.07      0.13       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.58      0.52      0.42      1523\n",
      "weighted avg       0.58      0.58      0.47      1523\n",
      "\n",
      "[[836  33]\n",
      " [608  46]]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "model score: 0.581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.94      0.72       869\n",
      "           1       0.56      0.11      0.18       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.57      0.52      0.45      1523\n",
      "weighted avg       0.57      0.58      0.49      1523\n",
      "\n",
      "[[813  56]\n",
      " [582  72]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.90      0.71       869\n",
      "           1       0.52      0.14      0.22       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.55      0.52      0.46      1523\n",
      "weighted avg       0.56      0.58      0.50      1523\n",
      "\n",
      "[[786  83]\n",
      " [563  91]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(),\n",
    "    SVC(random_state=42),\n",
    "    #NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    #MultinomialNB(),\n",
    "    #SGDClassifier(random_state=42),\n",
    "    #MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train.location, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test.location, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test.location)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that there is a really low f1-score for a target of 1 across all models\n",
    "\n",
    "TOP 3:\n",
    "\n",
    "AdaBoostClassifier | SVC | GradientBoostingClassifier |\n",
    "--- |--- |--- |\n",
    "0.581 | 0.579 |0.576 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.581\n"
     ]
    }
   ],
   "source": [
    "location_abc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('abc', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "location_abc.fit(X_train.location, y_train)\n",
    "print(\"model score: %.3f\" % location_abc.score(X_test.location, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc__n_estimators': 290}\n",
      "0.5830870279146141\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [279, 290, 291]\n",
    "\n",
    "parameters = {\n",
    "    \"abc__n_estimators\" : n_estimators\n",
    "}\n",
    "\n",
    "\n",
    "CV = GridSearchCV(location_abc, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train.location, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.97      0.72       869\n",
      "           1       0.59      0.06      0.11       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.59      0.51      0.42      1523\n",
      "weighted avg       0.58      0.58      0.46      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"model score: %.3f\" % CV.score(X_test.location, y_test))\n",
    "predictions = CV.predict(X_test.location)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of the tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='distance')\n",
      "model score: 0.564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.28      0.43       869\n",
      "           1       0.50      0.94      0.65       654\n",
      "\n",
      "    accuracy                           0.56      1523\n",
      "   macro avg       0.68      0.61      0.54      1523\n",
      "weighted avg       0.70      0.56      0.52      1523\n",
      "\n",
      "[[246 623]\n",
      " [ 41 613]]\n",
      "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.684\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.94      0.77       869\n",
      "           1       0.81      0.34      0.48       654\n",
      "\n",
      "    accuracy                           0.68      1523\n",
      "   macro avg       0.73      0.64      0.63      1523\n",
      "weighted avg       0.72      0.68      0.65      1523\n",
      "\n",
      "[[817  52]\n",
      " [429 225]]\n",
      "NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "      kernel='rbf', max_iter=-1, nu=0.5, probability=True, random_state=42,\n",
      "      shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       869\n",
      "           1       0.83      0.61      0.70       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.79      0.76      0.76      1523\n",
      "weighted avg       0.79      0.78      0.77      1523\n",
      "\n",
      "[[788  81]\n",
      " [258 396]]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best')\n",
      "model score: 0.699\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.77      0.75       869\n",
      "           1       0.67      0.60      0.63       654\n",
      "\n",
      "    accuracy                           0.70      1523\n",
      "   macro avg       0.69      0.69      0.69      1523\n",
      "weighted avg       0.70      0.70      0.70      1523\n",
      "\n",
      "[[672 197]\n",
      " [262 392]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "model score: 0.761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81       869\n",
      "           1       0.81      0.57      0.67       654\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.78      0.74      0.74      1523\n",
      "weighted avg       0.77      0.76      0.75      1523\n",
      "\n",
      "[[783  86]\n",
      " [278 376]]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "model score: 0.722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78       869\n",
      "           1       0.73      0.55      0.63       654\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.72      0.70      0.70      1523\n",
      "weighted avg       0.72      0.72      0.71      1523\n",
      "\n",
      "[[737 132]\n",
      " [292 362]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       869\n",
      "           1       0.66      0.66      0.66       654\n",
      "\n",
      "    accuracy                           0.71      1523\n",
      "   macro avg       0.70      0.70      0.70      1523\n",
      "weighted avg       0.71      0.71      0.71      1523\n",
      "\n",
      "[[646 223]\n",
      " [222 432]]\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "model score: 0.795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82       869\n",
      "           1       0.77      0.74      0.76       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.80      0.79      1523\n",
      "\n",
      "[[729 140]\n",
      " [172 482]]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.790\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       869\n",
      "           1       0.78      0.71      0.74       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.78      0.78      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[736 133]\n",
      " [187 467]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78       869\n",
      "           1       0.72      0.69      0.70       654\n",
      "\n",
      "    accuracy                           0.75      1523\n",
      "   macro avg       0.74      0.74      0.74      1523\n",
      "weighted avg       0.75      0.75      0.75      1523\n",
      "\n",
      "[[691 178]\n",
      " [205 449]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    #AdaBoostClassifier(random_state=42),\n",
    "    #GradientBoostingClassifier(random_state=42),\n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(random_state=42),\n",
    "    #MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train.text, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test.text, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test.text)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TOP 3\n",
    "\n",
    "MultinomialNB : 0.795\n",
    "SGDClassifier : 0.790\n",
    "NuSVC : 0.777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.795\n"
     ]
    }
   ],
   "source": [
    "text_mnb = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_mnb.fit(X_train.text, y_train)\n",
    "print(\"model score: %.3f\" % text_mnb.score(X_test.text, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mnb__alpha': 1}\n",
      "0.7796387520525452\n",
      "model score: 0.795\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'mnb__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(text_mnb, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train.text, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)\n",
    "print(\"model score: %.3f\" % CV.score(X_test.text, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "text_sgdc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('sgdc', SGDClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "text_sgdc.fit(X_train.text, y_train)\n",
    "print(\"model score: %.3f\" % text_sgdc.score(X_test.text, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sgdc__alpha': 0.001, 'sgdc__max_iter': 19}\n",
      "0.7876847290640394\n",
      "model score: 0.779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       869\n",
      "           1       0.81      0.64      0.71       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.79      0.76      0.77      1523\n",
      "weighted avg       0.78      0.78      0.77      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'sgdc__alpha': [1e-5, 1e-4, 1e-3], # learning rate\n",
    "    'sgdc__max_iter': [19, 20, 21], # number of epochs\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(text_sgdc, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train.text, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)\n",
    "\n",
    "print(\"model score: %.3f\" % CV.score(X_test.text, y_test))\n",
    "\n",
    "predictions = CV.predict(X_test.text)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all models into a meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions(X_input):\n",
    "    #supress a warning\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    text_pred = text_mnb.predict_proba(X_input.text)\n",
    "    location_pred = location_abc.predict_proba(X_input.location)\n",
    "    keyword_pred = keyword_dtc.predict_proba(X_input.keyword)\n",
    "\n",
    "    X_input['text_pred'] = text_pred[:,0]\n",
    "    X_input['location_pred'] = location_pred[:,0]\n",
    "    X_input['keyword_pred'] = keyword_pred[:,0] \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_predictions(X_train)\n",
    "add_predictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pred</th>\n",
       "      <th>location_pred</th>\n",
       "      <th>keyword_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>casualty</td>\n",
       "      <td>Massachusetts, USA</td>\n",
       "      <td>japan nuke program (albeit unsuccessful) casua...</td>\n",
       "      <td>0.204679</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>devastation</td>\n",
       "      <td>nan</td>\n",
       "      <td>utter shock devastation not go work left feeli...</td>\n",
       "      <td>0.794757</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>lava</td>\n",
       "      <td>nan</td>\n",
       "      <td>liked video &lt;number&gt; gaming &lt;url&gt; minecraft ps...</td>\n",
       "      <td>0.873592</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>blizzard</td>\n",
       "      <td>Ideally under a big tree</td>\n",
       "      <td>horrible moment open dryer looks like snowy bl...</td>\n",
       "      <td>0.542082</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>drown</td>\n",
       "      <td>new york</td>\n",
       "      <td>drown cannot swim &lt;url&gt;</td>\n",
       "      <td>0.769428</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          keyword                  location  \\\n",
       "1457     casualty        Massachusetts, USA   \n",
       "2746  devastation                       nan   \n",
       "4727         lava                       nan   \n",
       "847      blizzard  Ideally under a big tree   \n",
       "2896        drown                  new york   \n",
       "\n",
       "                                                   text  text_pred  \\\n",
       "1457  japan nuke program (albeit unsuccessful) casua...   0.204679   \n",
       "2746  utter shock devastation not go work left feeli...   0.794757   \n",
       "4727  liked video <number> gaming <url> minecraft ps...   0.873592   \n",
       "847   horrible moment open dryer looks like snowy bl...   0.542082   \n",
       "2896                            drown cannot swim <url>   0.769428   \n",
       "\n",
       "      location_pred  keyword_pred  \n",
       "1457       0.500286      0.394366  \n",
       "2746       0.500286      0.394366  \n",
       "4727       0.500286      0.394366  \n",
       "847        0.500286      0.394366  \n",
       "2896       0.503644      0.394366  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='distance')\n",
      "model score: 0.774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80       869\n",
      "           1       0.74      0.72      0.73       654\n",
      "\n",
      "    accuracy                           0.77      1523\n",
      "   macro avg       0.77      0.77      0.77      1523\n",
      "weighted avg       0.77      0.77      0.77      1523\n",
      "\n",
      "[[705 164]\n",
      " [180 474]]\n",
      "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       869\n",
      "           1       0.79      0.72      0.76       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "[[747 122]\n",
      " [182 472]]\n",
      "NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "      kernel='rbf', max_iter=-1, nu=0.5, probability=True, random_state=42,\n",
      "      shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84       869\n",
      "           1       0.81      0.72      0.76       654\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.81      0.80      0.80      1523\n",
      "weighted avg       0.81      0.81      0.80      1523\n",
      "\n",
      "[[760 109]\n",
      " [186 468]]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best')\n",
      "model score: 0.761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       869\n",
      "           1       0.73      0.71      0.72       654\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.76      0.75      0.76      1523\n",
      "weighted avg       0.76      0.76      0.76      1523\n",
      "\n",
      "[[694 175]\n",
      " [189 465]]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "model score: 0.764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       869\n",
      "           1       0.73      0.71      0.72       654\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.76      0.76      0.76      1523\n",
      "weighted avg       0.76      0.76      0.76      1523\n",
      "\n",
      "[[700 169]\n",
      " [190 464]]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "model score: 0.781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81       869\n",
      "           1       0.74      0.76      0.75       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.78      0.78      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n",
      "[[690 179]\n",
      " [155 499]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.780\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80       869\n",
      "           1       0.74      0.76      0.75       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.78      0.78      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n",
      "[[691 178]\n",
      " [157 497]]\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "model score: 0.798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84       869\n",
      "           1       0.83      0.66      0.74       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.78      0.79      1523\n",
      "weighted avg       0.80      0.80      0.79      1523\n",
      "\n",
      "[[783  86]\n",
      " [221 433]]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       869\n",
      "           1       0.78      0.72      0.75       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[737 132]\n",
      " [181 473]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       869\n",
      "           1       0.78      0.73      0.75       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[732 137]\n",
      " [178 476]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    #AdaBoostClassifier(random_state=42),\n",
    "    #GradientBoostingClassifier(random_state=42),\n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(random_state=42),\n",
    "    MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\"]])\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NuSVC : 0.806\n",
    "SVC : 0.800\n",
    "MultinomialNB : 0.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84       869\n",
      "           1       0.81      0.72      0.76       654\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.81      0.80      0.80      1523\n",
      "weighted avg       0.81      0.81      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_rfc = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('nusvc', NuSVC(random_state=42))\n",
    "])\n",
    "\n",
    "meta_rfc.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_train)\n",
    "print(\"model score: %.3f\" % meta_rfc.score(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_test))\n",
    "\n",
    "predictions = meta_rfc.predict(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\"]])\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nusvc__class_weight': {0: 0.1}, 'nusvc__kernel': 'rbf', 'nusvc__max_iter': -1}\n",
      "0.9011494252873563\n"
     ]
    }
   ],
   "source": [
    "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "class_weight = [{0: w} for w in [0.1, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9 ,1]]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"nusvc__kernel\" : kernel,\n",
    "    'nusvc__max_iter':[-1, 10, 20],\n",
    "    'nusvc__class_weight': class_weight,\n",
    "    \n",
    "}\n",
    "\n",
    "CV = GridSearchCV(meta_rfc, parameters, cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84       869\n",
      "           1       0.81      0.72      0.76       654\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.81      0.80      0.80      1523\n",
      "weighted avg       0.81      0.81      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_rfc = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('nusvc', NuSVC(random_state=42, class_weight = {0: 0.1}, kernel = 'rbf'))\n",
    "])\n",
    "\n",
    "meta_rfc.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_train)\n",
    "print(\"model score: %.3f\" % meta_rfc.score(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\"]], y_test))\n",
    "\n",
    "predictions = meta_rfc.predict(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\"]])\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
