{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/DLaux/BSA2020_Team_Tissot_Project_2/master/data/train.csv\", \n",
    "                          encoding='utf_8', \n",
    "                          dtype = 'unicode',\n",
    "                          parse_dates = True,\n",
    "                          infer_datetime_format = True,\n",
    "                          low_memory=False)\n",
    "\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/DLaux/BSA2020_Team_Tissot_Project_2/master/data/test.csv\", \n",
    "                          encoding='utf_8', \n",
    "                          dtype = 'unicode',\n",
    "                          parse_dates = True,\n",
    "                          infer_datetime_format = True,\n",
    "                          low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "# word_count\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# mean_word_length\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# unique_word_count\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# char_count\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# stop_word_count\n",
    "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id keyword location                                               text  \\\n",
       "0  1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1  4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2  5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "  target  word_count  mean_word_length  unique_word_count  char_count  \\\n",
       "0      1          13          4.384615                 13          69   \n",
       "1      1           7          4.571429                  7          38   \n",
       "2      1          22          5.090909                 20         133   \n",
       "\n",
       "   stop_word_count  \n",
       "0                6  \n",
       "1                0  \n",
       "2               11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocess_tweets import preprocess_tweet, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.text = df_train.text.apply(preprocess_tweet).apply(remove_stopwords)\n",
    "df_test.text  = df_test.text.apply(preprocess_tweet).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop([\"target\", \"id\"], axis =1)\n",
    "y = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = X[[\"word_count\", \"mean_word_length\", \"unique_word_count\", \"char_count\", \"stop_word_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[[\"word_count\", \"mean_word_length\", \"unique_word_count\", \"char_count\", \"stop_word_count\"]] = normalize(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keyword = X.keyword.astype(\"str\")\n",
    "X.location = X.location.astype(\"str\")\n",
    "X.text = X.text.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  keyword location                                               text  \\\n",
      "0     nan      nan       deeds reason earthquake may allah forgive us   \n",
      "1     nan      nan              forest fire near la ronge sask canada   \n",
      "2     nan      nan  residents asked shelter place notified officer...   \n",
      "3     nan      nan  <number> people receive wildfires evacuation o...   \n",
      "\n",
      "   word_count  mean_word_length  unique_word_count  char_count  \\\n",
      "0    0.181076          0.061073           0.181076    0.961097   \n",
      "1    0.177065          0.115634           0.177065    0.961210   \n",
      "2    0.160794          0.037209           0.146177    0.972075   \n",
      "3    0.120539          0.107355           0.120539    0.979382   \n",
      "\n",
      "   stop_word_count  \n",
      "0         0.083574  \n",
      "1         0.000000  \n",
      "2         0.080397  \n",
      "3         0.015067  \n",
      "----------------------------------------------------------------------\n",
      "keyword               object\n",
      "location              object\n",
      "text                  object\n",
      "word_count           float64\n",
      "mean_word_length     float64\n",
      "unique_word_count    float64\n",
      "char_count           float64\n",
      "stop_word_count      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.head(4))\n",
    "print(\"-\"*70)\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the target \n",
    "lab_enc = LabelEncoder()\n",
    "encoded_y = lab_enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lab_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=72, stratify=encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_text = CountVectorizer(min_df=2, max_df=0.75, encoding='utf-8')\n",
    "X_train_text_counts = count_vect_text.fit_transform(X_train.text)\n",
    "\n",
    "\n",
    "#We don't need min_df and max_df for the rest of the features since they are single words\n",
    "count_vect_location = CountVectorizer(encoding='utf-8')\n",
    "X_train_location_counts = count_vect_location.fit_transform(X_train.location.astype(\"str\"))\n",
    "\n",
    "count_vect_keyword = CountVectorizer(encoding='utf-8')\n",
    "X_train_keyword_counts = count_vect_keyword.fit_transform(X_train.keyword.astype(\"str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_text_tfidf = tfidf_transformer.fit_transform(X_train_text_counts)\n",
    "X_train_location_tfidf = tfidf_transformer.fit_transform(X_train_location_counts)\n",
    "X_train_keyword_tfidf = tfidf_transformer.fit_transform(X_train_keyword_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best')\n",
      "model score: 0.733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       869\n",
      "           1       0.68      0.71      0.70       654\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.73      0.73      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "[[652 217]\n",
      " [190 464]]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76       869\n",
      "           1       0.67      0.71      0.69       654\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.72      0.73      0.72      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "[[641 228]\n",
      " [187 467]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       869\n",
      "           1       0.67      0.71      0.69       654\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.72      0.72      0.72      1523\n",
      "weighted avg       0.73      0.72      0.73      1523\n",
      "\n",
      "[[636 233]\n",
      " [187 467]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    #NuSVC(probability=True, random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    #AdaBoostClassifier(random_state=42),\n",
    "    #GradientBoostingClassifier(random_state=42),\n",
    "    #MultinomialNB(),\n",
    "    SGDClassifier(random_state=42),\n",
    "    MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train.keyword, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test.keyword, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test.keyword)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It seems like that the DecisionTreeClassifier has the better model score without hyperparameter optimization.\n",
    "\n",
    "Top 3\n",
    "\n",
    "    DecisionTreeClassifier : 0.733\n",
    "    SGDClassifier :0.728\n",
    "    MLPClassifier : 0.724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.733\n"
     ]
    }
   ],
   "source": [
    "keyword_dtc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "keyword_dtc.fit(X_train.keyword, y_train)\n",
    "print(\"model score: %.3f\" % keyword_dtc.score(X_test.keyword, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       869\n",
      "           1       0.68      0.71      0.70       654\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.73      0.73      1523\n",
      "weighted avg       0.73      0.73      0.73      1523\n",
      "\n",
      "[[652 217]\n",
      " [190 464]]\n"
     ]
    }
   ],
   "source": [
    "predictions = keyword_dtc.predict(X_test.keyword)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dtc__max_depth': 85, 'dtc__min_samples_leaf': 0.001, 'dtc__min_samples_split': 2}\n",
      "0.7254515599343185\n"
     ]
    }
   ],
   "source": [
    "max_depth = [None, 85, 86, 87, 88, 89,90]\n",
    "min_samples_leaf = [0.001, 0.01, 0.1]\n",
    "min_samples_split = [2, 3, 4]\n",
    "\n",
    "parameters = {\n",
    "    \"dtc__max_depth\" : max_depth,\n",
    "    \"dtc__min_samples_split\" : min_samples_split,\n",
    "    \"dtc__min_samples_leaf\" : min_samples_leaf,\n",
    "}\n",
    "\n",
    "\n",
    "CV = GridSearchCV(keyword_dtc, parameters, n_jobs= 2)\n",
    "                  \n",
    "CV.fit(X_train.keyword, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.96      0.72       869\n",
      "           1       0.58      0.07      0.13       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.58      0.52      0.42      1523\n",
      "weighted avg       0.58      0.58      0.47      1523\n",
      "\n",
      "[[836  33]\n",
      " [608  46]]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "model score: 0.581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.94      0.72       869\n",
      "           1       0.56      0.11      0.18       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.57      0.52      0.45      1523\n",
      "weighted avg       0.57      0.58      0.49      1523\n",
      "\n",
      "[[813  56]\n",
      " [582  72]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.90      0.71       869\n",
      "           1       0.52      0.14      0.22       654\n",
      "\n",
      "    accuracy                           0.58      1523\n",
      "   macro avg       0.55      0.52      0.46      1523\n",
      "weighted avg       0.56      0.58      0.50      1523\n",
      "\n",
      "[[786  83]\n",
      " [563  91]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(),\n",
    "    SVC(random_state=42),\n",
    "    #NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    #MultinomialNB(),\n",
    "    #SGDClassifier(random_state=42),\n",
    "    #MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train.location, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test.location, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test.location)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that there is a really low f1-score for a target of 1 across all models\n",
    "\n",
    "TOP 3:\n",
    "\n",
    "AdaBoostClassifier | SVC | GradientBoostingClassifier |\n",
    "--- |--- |--- |\n",
    "0.581 | 0.579 |0.576 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.581\n"
     ]
    }
   ],
   "source": [
    "location_abc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('abc', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "location_abc.fit(X_train.location, y_train)\n",
    "print(\"model score: %.3f\" % location_abc.score(X_test.location, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc__n_estimators': 290}\n",
      "0.5830870279146141\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [279, 290, 291]\n",
    "\n",
    "parameters = {\n",
    "    \"abc__n_estimators\" : n_estimators\n",
    "}\n",
    "\n",
    "\n",
    "CV = GridSearchCV(location_abc, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train.location, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.54       869\n",
      "           1       0.46      0.58      0.51       654\n",
      "\n",
      "    accuracy                           0.53      1523\n",
      "   macro avg       0.53      0.53      0.53      1523\n",
      "weighted avg       0.54      0.53      0.53      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"model score: %.3f\" % CV.score(X_test.location, y_test))\n",
    "predictions = CV.predict(X_test.location)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of the tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "      kernel='rbf', max_iter=-1, nu=0.5, probability=True, random_state=42,\n",
      "      shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       869\n",
      "           1       0.84      0.60      0.70       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.79      0.75      0.76      1523\n",
      "weighted avg       0.79      0.78      0.77      1523\n",
      "\n",
      "[[792  77]\n",
      " [263 391]]\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "model score: 0.798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83       869\n",
      "           1       0.78      0.74      0.76       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "[[732 137]\n",
      " [171 483]]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.82       869\n",
      "           1       0.79      0.71      0.75       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.78      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[743 126]\n",
      " [190 464]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    #AdaBoostClassifier(random_state=42),\n",
    "    #GradientBoostingClassifier(random_state=42),\n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(random_state=42),\n",
    "    #MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train.text, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test.text, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test.text)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TOP 3\n",
    "\n",
    "MultinomialNB : 0.796\n",
    "SGDClassifier : 0.793\n",
    "NuSVC : 0.774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.798\n"
     ]
    }
   ],
   "source": [
    "text_mnb = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_mnb.fit(X_train.text, y_train)\n",
    "print(\"model score: %.3f\" % text_mnb.score(X_test.text, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mnb__alpha': 1}\n",
      "0.7798029556650247\n",
      "model score: 0.798\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'mnb__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(text_mnb, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train.text, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)\n",
    "print(\"model score: %.3f\" % CV.score(X_test.text, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.793\n"
     ]
    }
   ],
   "source": [
    "text_sgdc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('sgdc', SGDClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "text_sgdc.fit(X_train.text, y_train)\n",
    "print(\"model score: %.3f\" % text_sgdc.score(X_test.text, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sgdc__alpha': 0.001, 'sgdc__max_iter': 19}\n",
      "0.7908045977011494\n",
      "model score: 0.776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82       869\n",
      "           1       0.80      0.64      0.71       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.76      0.76      1523\n",
      "weighted avg       0.78      0.78      0.77      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'sgdc__alpha': [1e-5, 1e-4, 1e-3], # learning rate\n",
    "    'sgdc__max_iter': [19, 20, 21], # number of epochs\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(text_sgdc, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train.text, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)\n",
    "\n",
    "print(\"model score: %.3f\" % CV.score(X_test.text, y_test))\n",
    "\n",
    "predictions = CV.predict(X_test.text)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>0.181076</td>\n",
       "      <td>0.061073</td>\n",
       "      <td>0.181076</td>\n",
       "      <td>0.961097</td>\n",
       "      <td>0.083574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>0.177065</td>\n",
       "      <td>0.115634</td>\n",
       "      <td>0.177065</td>\n",
       "      <td>0.961210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>0.160794</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.146177</td>\n",
       "      <td>0.972075</td>\n",
       "      <td>0.080397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>&lt;number&gt; people receive wildfires evacuation o...</td>\n",
       "      <td>0.120539</td>\n",
       "      <td>0.107355</td>\n",
       "      <td>0.120539</td>\n",
       "      <td>0.979382</td>\n",
       "      <td>0.015067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>0.175684</td>\n",
       "      <td>0.049411</td>\n",
       "      <td>0.164703</td>\n",
       "      <td>0.966260</td>\n",
       "      <td>0.076862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  \\\n",
       "0     nan      nan       deeds reason earthquake may allah forgive us   \n",
       "1     nan      nan              forest fire near la ronge sask canada   \n",
       "2     nan      nan  residents asked shelter place notified officer...   \n",
       "3     nan      nan  <number> people receive wildfires evacuation o...   \n",
       "4     nan      nan  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "   word_count  mean_word_length  unique_word_count  char_count  \\\n",
       "0    0.181076          0.061073           0.181076    0.961097   \n",
       "1    0.177065          0.115634           0.177065    0.961210   \n",
       "2    0.160794          0.037209           0.146177    0.972075   \n",
       "3    0.120539          0.107355           0.120539    0.979382   \n",
       "4    0.175684          0.049411           0.164703    0.966260   \n",
       "\n",
       "   stop_word_count  \n",
       "0         0.083574  \n",
       "1         0.000000  \n",
       "2         0.080397  \n",
       "3         0.015067  \n",
       "4         0.076862  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train = X_train[[\"word_count\", \"unique_word_count\", \"mean_word_length\", \"char_count\"]]\n",
    "meta_features_test = X_test[[\"word_count\", \"unique_word_count\", \"mean_word_length\", \"char_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "model score: 0.624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.55      0.63       869\n",
      "           1       0.55      0.72      0.62       654\n",
      "\n",
      "    accuracy                           0.62      1523\n",
      "   macro avg       0.64      0.64      0.62      1523\n",
      "weighted avg       0.65      0.62      0.62      1523\n",
      "\n",
      "[[480 389]\n",
      " [183 471]]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.63       869\n",
      "           1       0.55      0.70      0.61       654\n",
      "\n",
      "    accuracy                           0.62      1523\n",
      "   macro avg       0.63      0.63      0.62      1523\n",
      "weighted avg       0.64      0.62      0.62      1523\n",
      "\n",
      "[[487 382]\n",
      " [195 459]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.58      0.63       869\n",
      "           1       0.55      0.67      0.60       654\n",
      "\n",
      "    accuracy                           0.62      1523\n",
      "   macro avg       0.62      0.63      0.62      1523\n",
      "weighted avg       0.63      0.62      0.62      1523\n",
      "\n",
      "[[504 365]\n",
      " [215 439]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    #NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    #MultinomialNB(),\n",
    "    #SGDClassifier(random_state=42),\n",
    "    MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(meta_features_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(meta_features_test, y_test))\n",
    "    \n",
    "    predictions = pipe.predict(meta_features_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.624\n"
     ]
    }
   ],
   "source": [
    "meta_abc = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('abc', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "meta_abc.fit(meta_features_train, y_train)\n",
    "print(\"model score: %.3f\" % meta_abc.score(meta_features_test, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
    "                   n_estimators=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc__n_estimators': 45}\n",
      "0.6274220032840723\n",
      "model score: 0.622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.56      0.63       869\n",
      "           1       0.55      0.71      0.62       654\n",
      "\n",
      "    accuracy                           0.62      1523\n",
      "   macro avg       0.63      0.63      0.62      1523\n",
      "weighted avg       0.64      0.62      0.62      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"abc__n_estimators\" : [45, 50, 55]\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(meta_abc, parameters,cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(meta_features_train, y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)\n",
    "\n",
    "print(\"model score: %.3f\" % CV.score(meta_features_test, y_test))\n",
    "\n",
    "predictions = CV.predict(meta_features_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators = 50 is the default value and the best when using gridsearch, so we don't change the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all models into a meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions(X_input):\n",
    "    #supress a warning\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    text_pred = text_mnb.predict_proba(X_input.text)\n",
    "    location_pred = location_abc.predict_proba(X_input.location)\n",
    "    keyword_pred = keyword_dtc.predict_proba(X_input.keyword)\n",
    "    meta_features_pred = meta_abc.predict_proba(X_input[[\"word_count\", \"unique_word_count\", \"mean_word_length\", \"char_count\"]])\n",
    "\n",
    "    X_input['text_pred'] = text_pred[:,0]\n",
    "    X_input['location_pred'] = location_pred[:,0]\n",
    "    X_input['keyword_pred'] = keyword_pred[:,0] \n",
    "    X_input['meta_features_pred'] = meta_features_pred[:,0] \n",
    "    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_predictions(X_train)\n",
    "add_predictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>text_pred</th>\n",
       "      <th>location_pred</th>\n",
       "      <th>keyword_pred</th>\n",
       "      <th>meta_features_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>casualty</td>\n",
       "      <td>Massachusetts, USA</td>\n",
       "      <td>japan nuke program albeit unsuccessful casualt...</td>\n",
       "      <td>0.160885</td>\n",
       "      <td>0.037229</td>\n",
       "      <td>0.146259</td>\n",
       "      <td>0.972620</td>\n",
       "      <td>0.073129</td>\n",
       "      <td>0.212221</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.501551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>devastation</td>\n",
       "      <td>nan</td>\n",
       "      <td>utter shock devastation not go work left feeli...</td>\n",
       "      <td>0.205398</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>0.197182</td>\n",
       "      <td>0.953046</td>\n",
       "      <td>0.098591</td>\n",
       "      <td>0.796248</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.508251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>lava</td>\n",
       "      <td>nan</td>\n",
       "      <td>liked video &lt;url&gt; minecraft postscript &lt;number...</td>\n",
       "      <td>0.183202</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.168546</td>\n",
       "      <td>0.967308</td>\n",
       "      <td>0.036640</td>\n",
       "      <td>0.879254</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.503712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>blizzard</td>\n",
       "      <td>Ideally under a big tree</td>\n",
       "      <td>horrible moment yoyou open yoyoup dryer looks ...</td>\n",
       "      <td>0.203032</td>\n",
       "      <td>0.027192</td>\n",
       "      <td>0.188529</td>\n",
       "      <td>0.957149</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.689005</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.507862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>drown</td>\n",
       "      <td>new york</td>\n",
       "      <td>drown cannot swim &lt;url&gt;</td>\n",
       "      <td>0.153079</td>\n",
       "      <td>0.092603</td>\n",
       "      <td>0.153079</td>\n",
       "      <td>0.969500</td>\n",
       "      <td>0.068035</td>\n",
       "      <td>0.763054</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.502759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          keyword                  location  \\\n",
       "1457     casualty        Massachusetts, USA   \n",
       "2746  devastation                       nan   \n",
       "4727         lava                       nan   \n",
       "847      blizzard  Ideally under a big tree   \n",
       "2896        drown                  new york   \n",
       "\n",
       "                                                   text  word_count  \\\n",
       "1457  japan nuke program albeit unsuccessful casualt...    0.160885   \n",
       "2746  utter shock devastation not go work left feeli...    0.205398   \n",
       "4727  liked video <url> minecraft postscript <number...    0.183202   \n",
       "847   horrible moment yoyou open yoyoup dryer looks ...    0.203032   \n",
       "2896                            drown cannot swim <url>    0.153079   \n",
       "\n",
       "      mean_word_length  unique_word_count  char_count  stop_word_count  \\\n",
       "1457          0.037229           0.146259    0.972620         0.073129   \n",
       "2746          0.030235           0.197182    0.953046         0.098591   \n",
       "4727          0.031657           0.168546    0.967308         0.036640   \n",
       "847           0.027192           0.188529    0.957149         0.079762   \n",
       "2896          0.092603           0.153079    0.969500         0.068035   \n",
       "\n",
       "      text_pred  location_pred  keyword_pred  meta_features_pred  \n",
       "1457   0.212221       0.500286      0.600000            0.501551  \n",
       "2746   0.796248       0.500286      0.171429            0.508251  \n",
       "4727   0.879254       0.500286      0.826087            0.503712  \n",
       "847    0.689005       0.500286      0.862069            0.507862  \n",
       "2896   0.763054       0.503644      0.892857            0.502759  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       869\n",
      "           1       0.80      0.72      0.76       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "[[749 120]\n",
      " [184 470]]\n",
      "NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "      kernel='rbf', max_iter=-1, nu=0.5, probability=True, random_state=42,\n",
      "      shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.83       869\n",
      "           1       0.79      0.70      0.75       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.78      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[750 119]\n",
      " [194 460]]\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "model score: 0.805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84       869\n",
      "           1       0.84      0.68      0.75       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.79      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "[[782  87]\n",
      " [210 444]]\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "model score: 0.780\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       869\n",
      "           1       0.73      0.77      0.75       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.78      0.78      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n",
      "[[684 185]\n",
      " [150 504]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       869\n",
      "           1       0.77      0.74      0.76       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[723 146]\n",
      " [168 486]]\n"
     ]
    }
   ],
   "source": [
    "classifiers = [   \n",
    "    #KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=42),\n",
    "    NuSVC(probability=True, random_state=42),\n",
    "    #DecisionTreeClassifier(random_state=42),\n",
    "    #RandomForestClassifier(random_state=42),\n",
    "    #AdaBoostClassifier(random_state=42),\n",
    "    #GradientBoostingClassifier(random_state=42),\n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(random_state=42),\n",
    "    MLPClassifier(random_state=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline([\n",
    "                     ('smote', SMOTE(random_state=42)),\n",
    "                     ('classifier', classifier)\n",
    "                     ])\n",
    "    pipe.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_test))\n",
    "    \n",
    "    predictions = pipe.predict(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]])\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NuSVC : 0.806\n",
    "SVC : 0.800\n",
    "MultinomialNB : 0.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84       869\n",
      "           1       0.84      0.68      0.75       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.79      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_mnb = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "meta_mnb.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_train)\n",
    "print(\"model score: %.3f\" % meta_mnb.score(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_test))\n",
    "\n",
    "predictions = meta_mnb.predict(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]])\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mnb__alpha': 1}\n",
      "0.8875205254515599\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"mnb__alpha\" : [1, 0.9, 0.5]    \n",
    "}\n",
    "\n",
    "CV = GridSearchCV(meta_mnb, parameters, cv = 3, n_jobs= -1)\n",
    "                  \n",
    "CV.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_train)  \n",
    "print(CV.best_params_) \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84       869\n",
      "           1       0.84      0.68      0.75       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.79      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_mnb = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('mnb', MultinomialNB(alpha = 1))\n",
    "     ])\n",
    "\n",
    "meta_mnb.fit(X_train[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_train)\n",
    "print(\"model score: %.3f\" % meta_mnb.score(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]], y_test))\n",
    "\n",
    "predictions = meta_mnb.predict(X_test[[\"text_pred\", \"location_pred\", \"keyword_pred\", \"meta_features_pred\"]])\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
